:imagesdir: ./images

== SSL Konfiguration für Vaultwarden

=== Motivation

Seit dem neusten Update des Bitwarden Browser Plugins, lassen sich nur noch verschlüsselte
Verbindungen zum Vaultwarden Server herstellen. Damit muss dieser unbedingt über ein SSL
Zertifikat verfügen um das HTTPS Protokoll zu unterstützen.

=== Konzept

Aus meiner Sicht gibt es 3 Lösungsmöglichkeiten:

1. Self-Signed Zertifikat + lokales Root-CA installieren

  (die sicherste und sauberste Lösung im Heimnetz)
  **Schritte**
  1. Eigenes Root-CA erstellen
     Auf deinem PC oder direkt im Unraid kannst du eine Root-CA generieren, z. B. mit mkcert oder OpenSSL.
     Beispiel mit mkcert (empfohlen):
     ```
     mkcert -install
     mkcert vaultwarden.local 192.168.178.50
     ```
     ➜ erzeugt:
    - Zertifikat: vaultwarden.local.pem
    - Key: vaultwarden.local-key.pem
    - Root-CA im Betriebssystem installiert

  2. Fritzbox DNS-Rebind anpassen
     Lege den Hostnamen intern fest:
     - In der Fritzbox → Heimnetz → Netzwerk
     - Deinem Unraid-Server einen Namen geben: z. B. unraid.local
     - (Falls .local Probleme macht, .lan verwenden: unraid.lan)
  3. Vaultwarden Container mit Zertifikaten starten
     In Unraid Docker Template:
     ```
     - WEB_VAULT_ENABLED=true
     - ROCKET_TLS={certs="/data/vaultwarden.local.pem",key="/data/vaultwarden.local-key.pem"}
     ```
     Zertifikate vorher in /mnt/user/appdata/vaultwarden/ speichern.
   4. Im Browser Plugin Hintergrunddomain ändern
      In Bitwarden-Plugineinstellungen:
         - "Selbstsigniertes Zertifikat zulassen"
         - passende URL https://vaultwarden.local

   ✔ Vorteile
   - Funktioniert offline, ohne Cloud.
   - Voll kompatibel mit Browser-Plugin.
   - Sicherheit wie bei professionellen internen Installationen.

   ✖ Nachteile
   - Du musst die Root-CA auf allen Geräten importieren (PC, Handy etc.)


2. Lokale DNS-Domain + Reverse Proxy + Let's Encrypt (DNS Challenge)

  (die Profi-Lösung mit "echten" Zertifikaten ohne öffentliche IP)
  **Benötigt**
  - Eine Domain (z. B. bei IONOS, Strato, Cloudflare)
  - Zugriff auf DNS
  **Schritte**
  1. Domain kaufen – Beispiel: schubert-home.de
  2. Subdomain: vaultwarden.schubert-home.de
  3. DNS Challenge konfigurieren (Cloudflare API oder IONOS API)
  4. In Unraid über SWAG oder Caddy Zertifikat automatisch generieren:
    - SWAG (Let’s Encrypt container)
    - oder Caddy (automatische Zertifikate via DNS-01)
  5. Reverse-Proxy leitet intern auf Vaultwarden weiter.

  ✔ Vorteile
  - Echte Let’s-Encrypt-Zertifikate
  - Automatische Erneuerung
  - Keine CA-Installation auf Clients nötig

  ✖ Nachteile
  - Domain kaufen
  - Einrichtung etwas komplex

3. Traefik oder Caddy als interner Proxy + interne CA

  (ideal wenn mehrere interne Dienste betrieben werden)
  - Container caddy oder traefik installieren
  - Dienst "vaultwarden.local" als Backend konfigurieren
  - Caddy kann sogar automatisch interne Zertifikate erzeugen
  - Browser-Plugin -> interne CA importieren

  ✔ Vorteile
  - Perfekt für Homeserver mit vielen Diensten
  - Caddy hat die eleganteste Konfiguration
  - Keine Zertifikate manuell herumkopieren

  ✖ Nachteile
  - Etwas Lernkurve

image:HomeserverVariante3.svg[]

Die grundlegende Änderung ist, dass ich nun 2 Server mit Resilio Sync bespiele aber jeweils verschieden konfiguriert.

Der Server den ich backupserver nenne, basiert auf einem Array ohne Pools und  besitzt nur eine Resilio Sync
Installation. Die synchronisierten Folder stellen die Master Folder dar und liegen unter /mnt/user/appdata/resiliosync/sync/folders/. Sie sind damit auch Teil des Backups welches von Appdata Backup
durchgeführt und auf einem eigenen Share abgelegt wird. Das Share kann von Windows nur als Read Only Share
eingehangen werden. Damit können die einzelnen Backupstände auf mobile USB Festplatten kopiert werden. Verändert oder
gelöscht werden können die Backups von Windows aus nicht. Darum kümmert sich Appdata Backup. Es löscht alle
Backups die älter sind als 7 Tage, lässt aber die jüngsten 3 immer stehen egal wie alt diese sind.
Das Backup ist auch so konfiguriert, dass zunächst alle Container gestoppt werden, dann das Backup erstellt wird und
dann die Container wieder gestartet werden. Beim Herunterfahren wird eine evtl. laufende Synchronisation unterbrochen,
so dass es nicht zu Inkonsistenzen kommt. Nachdem Backup, wird die Synchronisation dann wie gewohnt fortgesetzt.

Der Server den ich homeserver nenne, auf diesem konfiguriere ich den Pfad der synchronisierten Folder von Resilio
auf ein eigens angelegtes Share (/mnt/user/ablagen). Windows Nutzer, können dieses Share Schreibend und Lesend
einbinden und es ist nicht Teil der Backupläufe auf dem homeserver. Alle hier im Share liegenden Folder sind
synchronisierte Folder vom Backupserver.
Damit auch die Konfiguration des homeservers unter /mnt/user/appdata gesichert wird. Läuft auch hier Appdata Backup.
Das Backup stopped auch zunächst alle Container, führt dann das Backup durch und startet dann die Container wieder.
Die gesicherten Daten selbst schreibt das Backupprogramm in einen Unterordner des synchronisierten Folders backup-sync
auf dem von Resiolio benutzten und freigegebenen Share. Die Konfiguration des Backup funktioniert wie beim
backupserver: Alles über 7 Tage wird gelöscht außer die jüngsten 3.

Für die Sicherheit meiner Daten reicht mir das zunächst bis hier her. Ich denke die Daten sind aktuell sicher und
überleben auch einen Stromausfall bzw. haben einen solchen schon überlebt. Zu diesem Zwecke habe ich auf keinem
Server einen Pool eingerichtet. Meinen neuestem Wissenstand nach könnte man dies aber sogar ausfallsicher hinbekommen.
Aber das ist vielleicht noch ein Punkt für die Zukunft.

=== Erfahrungen und Erweiterungen

Da ich nach einer Weile die Datensicherheit für gut befand und ich meine Passwörter immer noch mit
Keypass sicherte und hier schon längst eine bessere Lösung suchte, stellte ich mir noch einen vaultserver hin.
Auf dem vaultserver habe ich eine vaultwarden Instanz eingerichtet, auf die ich per Browserplugin zugreifen kann.
Alle Daten liegen unter /mnt/home/appdata/vaultwarden. Das Backup des Server führe ich wieder mit Appdata Backup
durch. Wie immer werden erst alle Container gestoppt, dann die Daten gesichert und dann die Container wieder
gestartet. Die gesicherten Daten werden auf ein eigens erstelltes Share abgelegt. Es werden nur die jüngsten 3 Backups und alles was jünger als 7 Tage ist nicht gelöscht.

Diese gesicherten Daten sollten natürlich auch Teil des großen übergeordneten "Master" Backups auf dem backupserver
werden. Sie sollen dort in einen parallellen Ordner zu den Daten der homeserver Konfiguration gesichert werden. Ich muss sie also irgendwie zum backupserver übertragen. Das könnte ich über ein Remote Share welches ich in unraid
einhängen kann machen. Aber wenn es Netzwerkprobleme beim Transport gibt, scheitert das Backup. Außerdem muss der
Zielrechner an sein und eine asynchrone Speicherung wäre nicht möglich. Daher habe ich mich gegen das einhängen
eines Remote Shares vom backupserver entschieden. Stattdessen habe ich auch hier Resilio Sync installiert und das
genutzte Backupshare als synchronisierten Folder Pfad angegeben. Schon war das Problem gelöst. Jetzt kann der
Transport der Daten asynchron immer dann passieren, wenn es gerade möglich ist.

=== Fazit Variante 3

Was ziehe ich für ein Fazit? Bin ich zufrieder mit der Lösung?
Allgemein bin ich sehr zufrieden aber es gibt immer wieder Punkte über die ich nachdenke und bei denen ich meine,
sie weiter verbessern zu müssen.

1. Jedes Backup ist nur so gut wie sein Restore. Darum habe ich mir einen Laptop geschnappt das Backupverzeichnis
eingehangen und versucht das aktuelle Backup zu entpacken. Remote geht nicht. Erst lokal kopieren - ok dann wird der
Platz auf der Platte schon extrem eng. Jetzt entpacken - zu wenig Platz auf dem gesamten Laptop. Öffnen und einen
bestimmten Ordner später zum Entpacken selektieren wollen - dauert ewig beim Öffnen und scheitert dann mit zu wenig
Platz (vermutlich wird hier einfach auch erst komplett auf Platte entpackt). Also separate mobile USB Platte
rangestöpselt, das Backup drauf kopiert und dort entpackt - voila geht. Ich komme an alle Daten ran und könnte diese
wieder einspielen. Das ist verbesserungswürdig - wie weiß ich noch nicht. Aber Daten wie jpeg oder mp4, die schon
gepackt sind brauchen nicht nochmal gepackt werden. Das riesen Zip hab ich eigentlich nur, damit der Tranport übers
Netz schneller geht und bei einem FAT Zielsystem mir nicht gleich tausende Pfadwarnungen ala "zu lang" um die Ohren fliegen. Wenn das dann beim Entpacken passieren sollte, hab ich die Durststrecke Netzwerk schon mal hinter mir und kann weiter kopieren auf eine andere Platte mit ntfs oder so. Ziel ist vielleicht statt zip ein tar zu erstellen, mal schauen ob das mit Appdata Backup machbar ist.

2. Das Aufsetzen einer vaultwarden Instanz auf einem weiteren Thin Client aus einem Backup, hab ich ausprobiert nach Anleitung im Netz und hat super funktioniert.

3. Die Server sind sehr schwach und besitzen Lüfter, machen also leise Geräusche.
Der HP T620 geht gerade noch als vaultserver, den würde ich
aber zukünftig nicht wieder kaufen - viel zu schwach. Eine Lösung aus 3 Wyse wäre gut gewesen. Trotzdem ohne
Lüfter und mehr Speicher wäre auch cool.

4.  Mir fehlt immer noch eine Lösung für jellyfin - da brauch ich aber einen leistungsstarken Server. Mal
schauen und sparen.

5. Ich verwende nur USB Platten und habe beobachtet, dass die bei mir durchlaufen. Die legen sich nie schlafen
obgleich da nur an einigen Tagen was zu tun ist. Da muss ich die Konfiguration nochmal prüfen. Andererseits,
habe ich dadurch auch vielleicht bisher nicht die Probleme vor die alle Welt immer warnt. Es heißt immer, nimm
keine USB Platten. Unraid leistet auch keinen Support sowie USB Platten im Spiel sind. Außerdem scheint der
Unraid Support den Satz "Kein Backup, kein Mitleid" sehr ernst zu nehmen.

6. Ich würde zukünftig ger mal mehr mit einem Pool rumspielen. Zumal ich jetzt verstanden habe, dass man den
mit zfs wohl ausfallsicher hinbekommt.

7. Ich habe eine strikte Zeitvorgabe eingerichtet für automatische Tasks. Backup Homeserver 00:15 Montags,
Backup vaultserver 00:15 Dienstags.  Backup backupserver 00:15 Donnerstag. Damit ist das WE Backupfrei und
ich kann ungehindert basteln.

8. Backup und Perity Check. Zwischen einem Backup und einem Parity Check lasse ich bisher immer mindestens 48 h.
Die beiden Dinge dauern teilweise extrem lange und ich möchte nicht, dass beide Prozesse zufällig zeitgleich laufen.

9. Die Parity Platten werden bei unraid am meisten belastet, gleichzeitig sind das auch die teuersten weil am
größten und idealwerweise auch am schnellsten. Immer ein Auge drauf halten und beim Ausfall schnell wechseln.
Bei mir leben sie aber noch alle.

10. Ein Wort zu Updates und Upgrades. Ich habe das Backup so eingestellt, dass es automatisch vor dem Starten der Container noch Updates einspielt. Diese Updates beziehen sich aber nur auf die Docker Apps. Die Plugins
spiele ich selber ein. Hier warte ich immer erst eine Woche ab und dann spiele ich sie ein. Bisher keine
Probleme. Das Upgrade von unraid ist ein wichtiger Punkt. Dazu nehme ich mir Zeit. Das Teil lasse ich
mindestens eine Woche veröffentlicht sein und dann lese ich die Release Logs auch ganz genau. Meine Erfahrung
sagt mir das ist wichtig. Von 6.5 auf 7.0 hatte ich was falsch gemacht und das Upgrade hatte sich aufgehangen.
Neu booten hat geholfen aber das ist riskant. Lieber exakt nach Anweisung vorgehen.

11. Platten wechseln. Geht unproblematisch sofern die neue Platte gleich groß oder gößer ist. Danach läuft der
Parity Check wieder los und veruscht den Inhalt der neuen Platte aus der Parität herzustellen.

12. Eine Platte die ich aus einem Array entfernt hatte weil ich sie an meinem Laptop anstecken wollte und mal kurz alle Daten kopieren wollte (direkt per USB und nicht übers Netz) lies sich nicht lesen. Die Platten werden
von Windows gar nicht erkannt. Als ob sie nicht formatiert sind. Ich habe eine Weile gebraucht um zu schnallen,
dass das am Filesystem liegt, für das mein Windows vermutlich keine Treiber installiert hat. Unter unraid konnte
ich die Platte wieder super auslesen. Für solche Sachen hilft übrigens das Unassigned Device Plugin.










* xref:index.adoc[Zurück zur Übersicht]
